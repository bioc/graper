---
title: "Foundations of gprRR: Motivation and Derivation"
author: "Britta Velten"
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc: true
vignette: >
  %\VignetteIndexEntry{Foundations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

#Variational Bayes approach with a normal prior

Using a normal prior has the advantage of readily available conjugacies making inference with a variational Bayes approach simple. Can be extendend to inclusion of an additional sparsity promotin prior e.g. Spike and Slab.

Using only group-wise different hyperparameters, no covariance structure, i.e. $\gamma_k$ is hyperparameter for all $\beta_i$ with $i \in \mathcal{G}_i$.
(maybe see https://www.researchgate.net/publication/6307280_Variational_Bayesian_Approach_to_Canonical_Correlation_Analysis)

Use precision parameters for noise on y and for hyperpriors, modeled by a Gaussian prior to get conjugacy.

##Main questions
 
 - numerical issues with large marix inversion and zero determinant? ( For large design matrices (large n and/or large p) the determinant of $X^TX +diag(\gamma)$ evaluates as infitity. Thereby, the determinant of Sigma_beta becomes 0 after some iterations for large p or large n and with this entropy of beta goes to -infinity, estimation of tau does not work - probably also due to undererstimation of posterior variance)


###Hierarchical Model
$$\begin{align}
\tau &\sim\Gamma(r_\tau, d_\tau)\\
\gamma_k &\sim \Gamma(r_\gamma, d_\gamma) \quad \forall k\\
\beta_i|\gamma_i &\sim N(0, \frac{1}{\gamma_{g(i)}})\quad \forall i\\
y|\beta, \tau &\sim N(X\beta,\frac{1}{\tau}\mathbb{1})
\end{align}
$$

###Joint distribution
The joint distribution is then given by 
$$\begin{align}
p(y,\beta,\gamma,\tau)&=p(y|\beta)p(\beta|\gamma)p(\gamma)p(\tau)\\
&=\frac{1}{(2\pi)^\frac{n}{2}} \tau^\frac{n}{2} \exp\left(-\frac{\tau}{2}||y-X\beta||^2_2\right) 
\left(\prod_{i=1}^p \frac{\sqrt{\gamma_{g(i)}}}{\sqrt{2\pi}} \exp\left(-\frac{\gamma_{g(i)}}{2}\beta_i^2\right) \right)
\left(\prod_{k=1}^g \frac{d_\gamma^{r_\gamma}}{\Gamma(r_\gamma)} \gamma_k^{r_\gamma-1} \exp(-d_\gamma \gamma_k)\right)
\frac{d_\tau^{r_\tau}}{\Gamma(r_\tau)} \tau^{r_\tau-1} \exp(-d_\tau \tau)
\end{align}
$$

Hence,
$$\begin{align}
\log p(y,\beta,\gamma,\tau)&=\log p(y|\beta)+\log p(\beta|\gamma)+\log p(\gamma)+\log p(\tau)\\
&=const+ \frac{n}{2} \log(\tau) -\frac{\tau}{2}||y-X\beta||^2_2 +
\sum_{i=1}^p \left\{\frac{1}{2} \log(\gamma_{g(i)}) -\frac{\gamma_{g(i)}}{2}\beta_i^2 \right\}
+\sum_{k=1}^g \left\{(r_\gamma-1)  \log(\gamma_k ) -d_\gamma \gamma_k \right\}
+(r_\tau-1)  \log(\tau ) -d_\tau \tau
\end{align}
$$

### Variational Bayes and meanfield assumpiton
(For an introduction see e.g. http://people.inf.ethz.ch/bkay/talks/Brodersen_2013_03_22.pdf)
Given parameters $\theta$ and data y.
It can be shown that the log model evidence $\log(p(y))$ is decompsed into the Kullback-Leibler divergence between a density q on the parameters and the posterior $KL(q||p(\theta|y))$ and a "free energy term" $F(q,y)$. 
Variational Bayes methods are based on maximisation of the functional F w.r.t to q in order to obtain a tight lower bound on the log model evidence and minimize the KL-distane between the density q and the true (intractable) posterior.

To fix a class of distributions q over which to minimize, often a meanfield assumption is made, i.e. the density q factorizes in the indivdual parameters
$$q(\theta)=\prod_i q_i(\theta_i)$$

Then fixing all except one density $q_j$ the maximising function $q_j^*$ is given by

$$\log(q_j^*)(\theta_j)= \mathbb{E}_{-j}(\log(p(y,\theta))) -const$$

Here we assume

$$q(\beta,\gamma,\tau)=q(\beta) q(\gamma) q(\tau)$$.

Then the updates can be calculated as follows. (Due to the use of cojugate priors the normalization constant can be inferred from the known density form)

###Updates

####For $\beta$:
$$\begin{align}
\log q(\beta)&=const -\frac{\mathbb{E}_\tau(\tau)}{2}||y-X\beta||^2_2+
\sum_{i=1}^p \left\{-\frac{\mathbb{E}_\gamma(\gamma_{g(i)})}{2}\beta_i^2 \right\}
\end{align}
$$
Thus, $\beta \sim N(\mu_l, \Sigma_l)$ is a normal distribution with parameters...

$$\begin{align}
\mu_l &=\mathbb{E}_\tau(\tau)\Sigma^{(l)} X^t y \\
\Sigma^{(l)} &= (\mathbb{E}_\tau(\tau) X^t X +D)^{-1} \quad \text{with}\, D=diag(\mathbb{E}_\gamma(\gamma_{g(i)}))_i
\end{align}
$$

####For $\gamma$:
$$\begin{align}
\log q(\gamma)&=const+ 
\sum_{i=1}^p \left\{ \frac{1}{2}\log(\gamma_{g(i)}) -\frac{\gamma_{g(i)}}{2}\mathbb{E}_\beta(\beta_i^2) \right\}+
\sum_{k=1}^g \left\{(r_\gamma-1)  \log(\gamma_k ) -d_\gamma \gamma_k \right\}\\
&=const+\sum_{k=1}^g \left\{  \log(\gamma_k )  (r_\gamma-1 +\frac{1}{2}|\mathcal{G}_k|) - \gamma_k (d_\gamma+\frac{1}{2}\sum_{i\in\mathcal{G}_k}\mathbb{E}_\beta(\beta_i^2))\right\}
\end{align}
$$
Thus $\gamma_k \sim \Gamma(\alpha^{\gamma,k}_l, \beta^{\gamma,k}_l)$ are independent Gamma distributions with parameters given by
$$\begin{align}
\alpha^{\gamma,k}_l &= r_\gamma +\frac{1}{2}|\mathcal{G}_k|\\
\beta^{\gamma,k}_l &= d_\gamma+\frac{1}{2}\sum_{i\in\mathcal{G}_k}\mathbb{E}_\beta(\beta_i^2)
\end{align}
$$

####For $\tau$
$$\begin{align}
\log q(\tau)&= const+ \frac{n}{2} \log(\tau) -\frac{\tau}{2} \mathbb{E}_\beta||y-X\beta||^2_2
+(r_\tau-1)  \log(\tau ) -d_\tau \tau 
\end{align}
$$
Thus $\tau \sim \Gamma(\alpha^{\tau}_l, \beta^{\tau}_l)$ is  a Gamma distributions with parameters given by
$$\begin{align}
\alpha^{\tau}_l &= r_\tau +\frac{n}{2}\\
\beta^{\tau}_l &= d_\tau+\frac{1}{2}\mathbb{E}_\beta||y-X\beta||^2_2
\end{align}
$$

### Expected values required

$$
\begin{align}
\mathbb{E}_\tau{\tau}&=\frac{\alpha^{\tau}_l}{\beta^{\tau}_l}\\
&=\frac{ r_\tau +\frac{n}{2}}{d_\tau+\frac{1}{2}\mathbb{E}_\beta||y-X\beta||^2_2}\\
\\
\mathbb{E}_\gamma{\gamma_{k}}&=\frac{\alpha^{\gamma,k}_l}{\beta^{\gamma,k}_l}\\
&=\frac{r_\gamma +\frac{1}{2}|\mathcal{G}_k|}{d_\gamma+\frac{1}{2}\sum_{i\in\mathcal{G}_k}\mathbb{E}_\beta(\beta_i^2)}\\
\\
\mathbb{E}_\beta{\beta_i^2} &= \mu_l^2+\Sigma^{(l)}_{ii}\\
&=(\mathbb{E}_\tau(\tau)\Sigma^{(l)} X^t y)^2 +(\mathbb{E}_\tau(\tau) X^t X +D)^{-1}_{ii}\\
\\
\mathbb{E}_\beta||y-X\beta||_2^2 &=y^t y -2 y^t X \mathbb{E}_\beta \beta + \mathbb{E}_\beta (\beta^t X^t X \beta ) \\
&=y^t y -2 y^t X \mu_l + \sum_{i,j} (X^tX)_{i,j} (\Sigma^{(l)}_{i,j} +\mu^{(l)}_{i}\mu^{(l)}_{j}) 
\end{align}
$$

###Parameter Initialisation
To have a uninformative prior on $\tau$ and $\gamma$ default is $d_\tau=0.001, r_\tau=0.001, d_\gamma=0.001, r_\gamma=0.001$.

The required expected values are initialised by using the priors on $\gamma$ and $\tau$ and starting with updating $\beta$ based on those, such that this needs not to be initialised(?) Var initialisations to avoid local minima.

###Evidence lower bound

The evidence lower bound or "free energy" (in terminology used above) bounds the log model evidence from below and can be calculated in each step to observe convergence. Recall
$$\log(y)=\mathcal{L}(q)+KL(q||p)$$
with
$$
\begin{align}
\mathcal{L}(q)&=\mathbb{E}_q \left(\log \frac{p(y, \beta, \gamma, \tau)}{q(\beta, \gamma, \tau)}\right)\\
&=\mathbb{E}_q \left(\log p(y, \beta, \gamma, \tau)\right) +H(q(\beta, \gamma, \tau))\\
&=\mathbb{E}_q \left(\log p(y, \beta, \gamma, \tau)\right) +H(q(\beta)) + H(q(\gamma)) +H(q(\tau))
\end{align}
$$
where $H(q)=\int - q(\theta) \log q(\theta) d\theta$ denotes the differential entropy.

####joint density
$$
\begin{align}
\mathbb{E}_q \log p(y, \beta, \gamma, \tau)&=\mathbb{E}_q \log p(y| \beta, \tau) +\mathbb{E}_q \log p(\beta| \gamma) +\mathbb{E}_q \log p(\gamma)+\mathbb{E}_q \log p(\tau)
\end{align}
$$
with
$$
\begin{align}
\mathbb{E}_q \log p(y| \beta) &= \frac{n}{2} \mathbb{E} \log(\tau) -\frac{1}{2}\mathbb{E}\tau ||y-X\beta||_2^2-\frac{n}{2} \log(2\pi)\\
\mathbb{E}_q \log p(\beta| \gamma) &= \sum_i \frac{1}{2} \mathbb{E} \log(\gamma_{g(i)}) - \frac{1}{2} \mathbb{E} \gamma_{g(i)}\beta_i^2 - \frac{1}{2} \log(2\pi)\\ 
\mathbb{E}_q \log p(\gamma) &=\sum_k (r_\gamma-1)\mathbb{E} \log(\gamma_k) - d_\gamma \mathbb{E} \gamma_k - \log(\Gamma((r_\gamma)) + r_\gamma \log(d_\gamma) \\
\mathbb{E}_q \log p(\tau) &=(r_\tau-1)\mathbb{E} \log(\tau) - d_\tau \mathbb{E} \tau - \log(\Gamma((r_\tau)) + r_\gamma \log(d_\tau)
\end{align}
$$

The expectation factroize as q is assumed to facotrize, hence using the known ditirbutions and parameters of the variational density in each iteration one gets
$$
\begin{align}
\mathbb{E} \log(\tau)&=\psi(\alpha_\tau) - \log(\beta_\tau)\\
\mathbb{E}\tau ||y-X\beta||_2^2=\mathbb{E}\tau \mathbb{E}||y-X\beta||_2^2&=\frac{\alpha^{\tau}_l}{\beta^{\tau}_l}(y^t y -2 y^t X \mu_l + \sum_{i,j} (X^tX)_{i,j} (\Sigma^{(l)}_{i,j} +\mu^{(l)}_{i}\mu^{(l)}_{j}) )\\
\mathbb{E} \log(\gamma_{g(i)})&=\psi(\alpha_{\gamma,g(i)}) - \log(\beta_{\gamma,g(i)})\\
\mathbb{E} \gamma_{g(i)}\beta_i^2 =\mathbb{E} \gamma_{g(i)}\mathbb{E}\beta_i^2&=\frac{\alpha^{\gamma,g(i)}_l}{\beta^{\gamma,g(i)}_l}(\mu_l^2+\Sigma^{(l)}_{ii})\\
\mathbb{E} \gamma_k&=\frac{\alpha^{\gamma,k}_l}{\beta^{\gamma,k}_l}\\
\mathbb{E} \tau&=\frac{\alpha^{\tau}_l}{\beta^{\tau}_l}
\end{align}
$$

####entropies
Using the known expression for entropy (with the natural logarithm) of the gamma and multivaraite normal distribution one obtains
$$
\begin{align}
H(q(\beta)) &=\frac{p}{2}(\log(2\pi)+1)+\frac{1}{2} \log(|\Sigma^{(l)}|)\\
H(q(\gamma)) &=\sum_k \alpha^{\gamma,k}_l - \log(\beta^{\gamma,k}_l) + \log (\Gamma(\alpha^{\gamma,k}_l)) + (1-\alpha^{\gamma,k}_l)\psi(\alpha^{\gamma,k}_l)\\
H(q(\tau)) &=\alpha^{\tau}_l - \log(\beta^{\tau}_l) + \log (\Gamma(\alpha^{\tau}_l)) + (1-\alpha^{\tau}_l)\psi(\alpha^{\tau}_l)
\end{align}
$$
where $\psi$ denotes the digamma function $\psi(x)=\frac{\Gamma'(x)}{\Gamma(x)}$.


###Numerical issues
For large p the matrix inversion $(\mathbb{E}_\tau(\tau) X^t X +D)^{-1}$ becomes very slow. In ridge regression the problem is solev very fast with singular value decomposition of X.
$$
\begin{align}
X&=U\Delta V^T\\
(X^TX+\lambda\mathds{1})^-1&=(V\Delta^2 V^T+\lambda VV^T)^-1\\
&=(V(\Delta^2++\lambda mathds{1})V^T)^{-1}\\
&=(V(\Delta^2++\lambda mathds{1})^{-1}V^T)
\end{align}
$$
(can be even further simplified in the expression fot the mean)

However, this does not work if the entries on the diagonal of D are different (here group-wise). This problem should also occur in ARD settings. How is it solved here?

###Implementation
see fit_grpRR_full

## Fully facotrized model in beta

Here the same hierarchical model as in the previous section is assumed, but with a different meanfield assumption for the VB method:

$$q(\beta,\gamma,\tau)= \prod_i q(\beta_i) q(\gamma) q(\tau)$$.


Factorizing the variational density of $\beta$ fully lead to the follwing distributions in each update, but still assume full factorization.

$$
\begin{align}
\beta_i \sim N(\mu_i, \sigma^2_i)
\end{align}
$$
with
$$
\begin{align}
\sigma_i^2&=(\mathbb{E} \tau ||X_{\cdot,i}||_2^2 +\mathbb{E}\gamma_{g(i)})^{-1}\\
\mu_i&=\sigma_i^2*\mathbb{E} \tau  \left(X_{\cdot,i}^T y -\sum_{k=1}^n \sum_{j\neq i}^p X_{ki}X_{kj} \mathbb{E} \beta_j\right)
\end{align}
$$

In the other updates the expected value of $||y-X\beta||^2$ simplifies:

$$
\begin{align}
\mathbb{E}_\beta||y-X\beta||_2^2 &=y^t y -2 y^t X \mathbb{E}_\beta \beta + \mathbb{E}_\beta (\beta^t X^t X \beta ) \\
&=y^t y -2 y^t X \mu_l + \sum_{i,j} (X^tX)_{i,j} (diag(\sigma^2_l) +\mu^{(l)}_{i}\mu^{(l)}_{j}) 
\end{align}
$$
In the calculation of the ELB only the entropy of beta changes
$$H(q(\beta))= \sum_j H(q(\beta_j)) =\sum_j \frac{1}{2} \log (2\sigma_j^2 \pi e)$$

###Implementation
see fit_grpRR_fac

##Groupwise Factorization
As an intermediate between full and no factorization try to use group-wise factorization. Then svd approaches can then be used for matrix inversions, maybe TRY RANDOMIZED SVD FOR SPEED-UP.

Same hierarchical model, but with a different meanfield assumption for the VB method:

$$q(\beta,\gamma,\tau)= \prod_{g \in G} q(\beta_{i\in g}) q(\gamma) q(\tau)$$.


This leads to the follwing distributions in each update:
Writing $\beta_g=(\beta_i)_{i \in g}$ and for matrices $A_{gg}=(A_{ij})_{i,j \in g}$.

$$
\begin{align}
\beta_g \sim N(\mu_g, \Sigma_g)
\end{align}
$$
with
$$
\begin{align}
\Sigma_g&=(\mathbb{E} \tau \, (X^TX)_{g,g} +\mathbb{E}\gamma_{g})^{-1}\\
\mu_g&=\mathbb{E}\tau \Sigma_g ((X_{\cdot,g})^Ty -\mathbb{E}\beta_{g^c}(X^TX)_{g^c,g})
\end{align}
$$

In the other updates the expected value of $||y-X\beta||^2$ simplifies due to a blcok strucutre of the full covariance according to the groups.

###Implementation
see fit_grpRR_grpfac


#Variational Bayes approach with a spike and slab prior

To allow sparisty within groups we can replace the normal prior on the coefficients $\beta$ by a spike and slab prior $\beta \sim \pi N(0,\gamma) +(1-\pi)\delta_0$.
For variational inference this has been done e.g. in [Titias and Laszaro-Gredialla](http://www.tsc.uc3m.es/~miguel/papers/vmtmkl_nips.pdf): For application of variational Bayes methods the prior is reparametrized as a product of a Bernoulli and a normal randome variable using $\beta=s \tilde{\beta}$ with $s \sim Ber(\pi)$ and $\beta \sim N(0,\gamma)$. Due to the strong interaction between $s$ and $\tilde{\beta}$ Titias et al highlight the importance of ont seperating these two variables in the mean-field appraoximation.
To allow differntial penalization of groups of feautres we allow $\pi$ and $\gamma$ to depend on the group and aim at learning them as well.

##The Model

$$\begin{align}
\tau &\sim\Gamma(r_\tau, d_\tau)\\
\gamma_k &\sim \Gamma(r_\gamma, d_\gamma) \quad \forall k\\
\pi_k &\sim Beta(d_\pi, r_\pi) \quad \forall k\\
\beta_i|\gamma_{g(i)}, \pi_{g(i)} &\sim Bernoulli-Gaussian(\gamma_{g(i)}, \pi_{g(i)})\\
y|\beta, \tau &\sim N(X\beta,\frac{1}{\tau}\mathbb{1})
\end{align}
$$
or
$$\begin{align}
\tau &\sim\Gamma(r_\tau, d_\tau)\\
\gamma_k &\sim \Gamma(r_\gamma, d_\gamma) \quad \forall k\\
\pi_k &\sim Beta(d_\pi, r_\pi) \quad \forall k\\
\tilde{\beta_i}|\gamma_{g(i)} &\sim N(0,\frac{1}{\gamma_{g(i)}})\quad \forall i\\
s_i|\pi_{g(i)})&\sim Ber(\pi_{g(i)})\quad \forall i\\
\beta_i&=s_i \tilde{\beta_i} \quad \forall i\\
y|\beta, \tau &\sim N(X\beta,\frac{1}{\tau}\mathbb{1})
\end{align}
$$

##The joint distribution
$$\begin{align}
p(y,\tilde{\beta},s,\gamma,\tau,\pi)=p(y|\tilde{\beta},s, \tau) p(\tilde{\beta},s|\pi,\gamma)p(\pi)p(\gamma)p(\tau)
\end{align}
$$

$$\begin{align}
\log p(y,\tilde{\beta},s,\gamma,\tau,\pi) &= const + \frac{n}{2}\log(\tau)-\frac{\tau}{2}||y-X(\tilde{\beta}\bullet s)||^2_2 \\
&+\sum_{i=1}^p \log(\pi_{g(i)})s_i+\log(1-\pi_{g(i)})(1-s_i)\\
&+\sum_{i=1}^p\frac{1}{2}\log(\gamma_{g(i)})-\frac{\gamma_{g(i)}}{2}\tilde{\beta_i^2}\\
&+\sum_{k=1}^g \left\{(r_\gamma-1)  \log(\gamma_k ) -d_\gamma \gamma_k \right\}\\
&+\sum_{k=1}^g \left\{(d_\pi-1)  \log(\pi_k ) +(r_\pi-1)  \log(1-\pi_k ) -\log(B(d_\pi,r_\pi)\right\}\\
&+(r_\tau-1)  \log(\tau ) -d_\tau \tau
\end{align}
$$

##Mean-field approximation

$$
q(\tilde{\beta},\gamma,s,\tau,\pi)=q(\tilde{\beta},s)q(\tau)q(\gamma)q(\pi)
$$

##Updates

The updates are given by
$$
\log(q_j(\theta_j))=\mathbb{E}_{-j}\log(p(y,\theta))
$$

###For $\beta$: $\tilde{\beta}$ and $s$
$$\begin{align}
\log q(\tilde{\beta}, s)&=const -\frac{\mathbb{E}_\tau \tau}{2}||y-X(\tilde{\beta}\bullet s)||^2_2 +\\
&+\sum_{i=1}^p [\mathbb{E}_\pi\log(\pi_{g(i)})-\mathbb{E}_\pi \log(1-\pi_{g(i)}) ] s_i - \sum_{i=1}^p\frac{\mathbb{E}_\gamma \gamma_{g(i)}}{2}\tilde{\beta_i^2}
\end{align}
$$

Given $s=s_0$ we have  $\tilde{\beta}|s=s^* \sim N(\mu_l(s^*), \Sigma_l(s^*))$ with

$$\begin{align}
\mu_l(s^*) &=\mathbb{E}_\tau(\tau) \Sigma_l(s^*) X^*(s^*)^t y \\
 \Sigma_l(s^*)&= (\mathbb{E}_\tau(\tau) X^*(s^*)^t X^*(s^*) +D)^{-1} \quad \text{with}\, D=diag(\mathbb{E}_\gamma(\gamma_{g(i)}))_i
\end{align}
$$
where $X^*(s^*)$ is given by $X^*(s^*)_{\cdot,i} =X_{\cdot,i} \mathbb{1}_{s^*_i=1}$, i.e. columns in X corresponding to zero entries in $s^*$ are set to zero.
(Thus, we have a mixture of $2^p$ normal components)
The marginal for $s$ is given by
$$
q(s)=const \exp\left(\sum_{i=1}^p s_i \mathbb{E}\log\frac{\pi_{g(i)}}{1-\pi_{g(i)}}\right) det(\Sigma)^{\frac{1}{2}} \exp(-\frac{1}{2} \mu^T \Sigma^{-1} \mu)
$$
where
$$\begin{align}
\mu=\mu(s) &=\mathbb{E}_\tau(\tau) \Sigma(s) X(s)^t y \\
 \Sigma= \Sigma(s)&= (\mathbb{E}_\tau(\tau) X(s)^t X(s) +D)^{-1} \quad \text{with}\, D=diag(\mathbb{E}_\gamma(\gamma_{g(i)}))_i
\end{align}
$$

#### Notes on the update for s:
In a related model with latent variables Tistias et al give a formula in the supplementary for the marginal of s, i.e. $P(s=1)$, in a model fully facotrized over the features. To get to their formula the following steps are required:

1. Calculate $q(s,\beta)$ from the general form of the VB update as above
2. Get $q(s)=\int q(s,\beta) d\beta$ by re-wirting the terms in the integral as a normal density in $\beta$ (conditional density $\beta|s$) (integrating to 1). Here, it is important to complement correclty with all quantities depending on s, e.g. normalization constant of the resulting normal and quadratic term.
3. Re-write the resulting density as $const*\exp(s \xi)$ making use of $s=1$ or $s=0$ (case studies!)
4. Compare to a Bernoulli density $s\sim Ber(\psi)$, i.e.  $p(s)=(1-\psi) (\frac{\psi}{1-\psi})^s$. Hence, $\log(\frac{\psi}{1-\psi}) = logit(\psi)=\xi$ and $\psi=logit^{-1}(\xi)$.

###For $\gamma$
$$\begin{align}
\log q(\gamma)&=const+ 
\sum_{i=1}^p \left\{ \frac{1}{2}\log(\gamma_{g(i)}) -\frac{\gamma_{g(i)}}{2}\mathbb{E}_\tilde{\beta}(\tilde{\beta}_i^2) \right\}+
\sum_{k=1}^g \left\{(r_\gamma-1)  \log(\gamma_k ) -d_\gamma \gamma_k \right\}\\
&=const+\sum_{k=1}^g \left\{  \log(\gamma_k )  (r_\gamma-1 +\frac{1}{2}|\mathcal{G}_k|) - \gamma_k (d_\gamma+\frac{1}{2}\sum_{i\in\mathcal{G}_k}\mathbb{E}_\tilde{\beta}(\tilde{\beta}_i^2))\right\}
\end{align}
$$
Thus $\gamma_k \sim \Gamma(\alpha^{\gamma,k}_l, \beta^{\gamma,k}_l)$ are independent Gamma distributions with parameters given by
$$\begin{align}
\alpha^{\gamma,k}_l &= r_\gamma +\frac{1}{2}|\mathcal{G}_k|\\
\beta^{\gamma,k}_l &= d_\gamma+\frac{1}{2}\sum_{i\in\mathcal{G}_k}\mathbb{E}_\tilde{\beta}(\tilde{\beta}_i^2)
\end{align}
$$

###For $\tau$
$$\begin{align}
\log q(\tau)&= const+ \frac{n}{2} \log(\tau) -\frac{\tau}{2} \mathbb{E}_\beta||y-X\beta||^2_2
+(r_\tau-1)  \log(\tau ) -d_\tau \tau 
\end{align}
$$
Thus $\tau \sim \Gamma(\alpha^{\tau}_l, \beta^{\tau}_l)$ is  a Gamma distributions with parameters given by
$$\begin{align}
\alpha^{\tau}_l &= r_\tau +\frac{n}{2}\\
\beta^{\tau}_l &= d_\tau+\frac{1}{2}\mathbb{E}_\beta||y-X\beta||^2_2
\end{align}
$$

###For $\pi$
$$\begin{align}
\log q(\pi)&= const+ \sum_{i=1}^p \log(\pi_{g(i)})\mathbb{E}_s s_i+\log(1-\pi_{g(i)})(1-\mathbb{E}_s s_i)+\sum_{k=1}^g \left\{(d_\pi-1)  \log(\pi_k ) +(r_\pi-1)  \log(1-\pi_k ) -\log(B(d_\pi,r_\pi)\right\}\\
&=\sum_{k=1}^g \log(\pi_k) (d_\pi-1 +\sum_{i\in\mathcal{G}_k} \mathbb{E}_s s_i) +  \log(1-\pi_k )(r_\pi-1 +\sum_{i\in\mathcal{G}_k} 1 - \mathbb{E}_s s_i)\\
\end{align}
$$
Thus $\pi \sim Beta(\alpha^{\pi}_l, \beta^{\pi}_l)$ with parameters given by
$$\begin{align}
\alpha^{\pi}_l &=d_\pi +\sum_{i\in\mathcal{G}_k} \mathbb{E}_s s_i\\
\beta^{\pi}_l &= r_\pi +\sum_{i\in\mathcal{G}_k} (1 - \mathbb{E}_s s_i)
\end{align}
$$


##Problems
The multi-variate spike nd slab leads to a complicated marginal distribution of the Bernoulli-Variable making updates difficult....

## Expected values required


#Variational Bayes approach with a spike and slab prior - fully factorised

##Model and Joint distribution
As above.
$$\begin{align}
\log p(y,\tilde{\beta},s,\gamma,\tau,\pi) &= const + \frac{n}{2}\log(\tau)-\frac{\tau}{2}||y-X(\tilde{\beta}\bullet s)||^2_2 \\
&+\sum_{i=1}^p \log(\pi_{g(i)})s_i+\log(1-\pi_{g(i)})(1-s_i)\\
&+\sum_{i=1}^p\frac{1}{2}\log(\gamma_{g(i)})-\frac{\gamma_{g(i)}}{2}\tilde{\beta_i^2}\\
&+\sum_{k=1}^g \left\{(r_\gamma-1)  \log(\gamma_k ) -d_\gamma \gamma_k \right\}\\
&+\sum_{k=1}^g \left\{(d_\pi-1)  \log(\pi_k ) +(r_\pi-1)  \log(1-\pi_k ) -\log(B(d_\pi,r_\pi)\right\}\\
&+(r_\tau-1)  \log(\tau ) -d_\tau \tau
\end{align}
$$

##Mean-field assumption

$$
q(\tilde{\beta},\gamma,s,\tau,\pi)=q(\tau)q(\gamma)q(\pi) \prod_j q(\tilde{\beta_j},s_j)
$$

##Updates
The updates are given by
$$
\log(q_j(\theta_j))=\mathbb{E}_{-j}\log(p(y,\theta))
$$

###For $\beta$: $\tilde{\beta}$ and $s$

$$\begin{align}
\log(q(\tilde{\beta}_i, s_i)) &= const- \mathbb{E}\frac{\tau}{2} \mathbb{E}_{-i}||y-X(\tilde{\beta}\bullet s)||^2_2 \\
&+\mathbb{E}\log\frac{\pi_{g(i)}}{1-\pi_{g(i)}} s_i -\frac{\mathbb{E}\gamma_{g(i)}}{2}\tilde{\beta_i^2}
\end{align}
$$
We can write
$$
 \mathbb{E}_{-i}||y-X(\tilde{\beta}\bullet s)||^2_2 = const + \tilde{\beta_i} s_i \sum_k(-2y_kX_{ki} + 2 \sum _{j\neq i} X_{kj}X_{ki} \mathbb{E}(s_j \tilde{\beta_j})) + s_i \tilde{\beta_i}^2 \sum_k X_{ki}^2
 $$
Therefore the distribution of $\beta_i$ is a mixture of two normal disrtibutions q(\beta_i) = q(s=0)q(\beta_i|s=0)+q(s=1)q(\beta_i|s=1), where $\beta_i|s=0\sim N(0,\mathbb{E}\gamma_{g(i)}^{-1})$ and $\beta_i|s=1$ with parameters as below with $s_i=1$.
$$
\begin{align}
\beta_i|s \sim N(\mu_i(s), \sigma^2_i(s))
\end{align}
$$
with
$$
\begin{align}
\sigma_i^2(s_i) &= (s_i\mathbb{E} \tau ||X_{\cdot,i}||_2^2 +\mathbb{E}\gamma_{g(i)})^{-1}\\
\mu_i(s_i) &= s_i \sigma_i^2 \mathbb{E} \tau  \left( -\sum_{k=1}^n \sum_{j\neq i}^p X_{ki} X_{kj} \mathbb{E} (\tilde{\beta}_j s_j) + X_{\cdot,i}^T y \right)
\end{align}
$$

The marginal distribution of s_i is given by
$$
\begin{align}
s_i \sim Ber(\psi_i)
\end{align}
$$

with 
$$
\begin{align}
\psi_i&= logit^{-1}\left\{\mathbb{E}\log\frac{\pi_{g(i)}}{1-\pi_{g(i)}} -  
\frac{1}{2}  \log(\mathbb{E} \tau ||X_{\cdot,i}||_2^2 +\mathbb{E}\gamma_{g(i)}) + \frac{1}{2}  \log(\mathbb{E}\gamma_{g(i)}) +
\frac{( \mathbb{E} \tau)^2  \left(X_{\cdot,i}^T y -\sum_{k=1}^n \sum_{j\neq i}^p X_{ki} X_{kj} \mathbb{E} (\tilde{\beta}_j s_j) \right)^2}{2 (\mathbb{E} \tau ||X_{\cdot,i}||_2^2 +\mathbb{E}\gamma_{g(i)})} \right\}\\
&= logit^{-1}\left\{\mathbb{E}\log\frac{\pi_{g(i)}}{1-\pi_{g(i)}} -  
\frac{1}{2}  \log\left(||X_{\cdot,i}||_2^2 +\frac{\mathbb{E}\gamma_{g(i)}}{\mathbb{E} \tau }\right) + \frac{1}{2}  \log\left(\frac{\mathbb{E}\gamma_{g(i)}}{\mathbb{E} \tau }\right) +
\frac{ \mathbb{E} \tau}{2} \frac{  \left(X_{\cdot,i}^T y -\sum_{k=1}^n \sum_{j\neq i}^p X_{ki} X_{kj} \mathbb{E} (\tilde{\beta}_j s_j) \right)^2}{ ( ||X_{\cdot,i}||_2^2 +\frac{\mathbb{E}\gamma_{g(i)}}{\mathbb{E} \tau })} \right\}\\
\end{align}
$$

as with $\varphi(\cdot; \mu,\sigma^2)$ denoting the normal density
$$
\begin{align}
q(s_i)&= \int q(\tilde{\beta}_i,s_i) d\tilde{\beta}_i \\
&= const \exp\left(\mathbb{E}\log\frac{\pi_{g(i)}}{1-\pi_{g(i)}} s_i \right) \int \exp \left(- \mathbb{E}\frac{\tau}{2} \mathbb{E}_{-i}||y-X(\tilde{\beta}\bullet s)||^2_2 -\frac{\mathbb{E}\gamma_{g(i)}}{2}\tilde{\beta_i^2}\right) d\tilde{\beta}_i \\
&= const \exp\left(\mathbb{E}\log\frac{\pi_{g(i)}}{1-\pi_{g(i)}} s_i \right) \int \varphi(\beta_i; \mu_i(s_i), \sigma_i^2(s_i)) \sqrt{2\pi\sigma_i^2(s_i)} \exp\left( \frac{\mu_i(s_i)^2}{2\sigma_i^2(s_i)} \right) d\tilde{\beta}_i \\
&= const \exp\left(\mathbb{E}\log\frac{\pi_{g(i)}}{1-\pi_{g(i)}} s_i \right) \sqrt{\sigma_i^2(s_i)} \exp\left( \frac{\mu_i(s_i)^2}{2\sigma_i^2(s_i)} \right)\cdot 1\\
&= const \exp\left(\mathbb{E}\log\frac{\pi_{g(i)}}{1-\pi_{g(i)}} s_i +\frac{1}{2} \log{\sigma_i^2(s_i)}  +\frac{\mu_i(s_i)^2}{2\sigma_i^2(s_i)} \right)\\
\Rightarrow \log q(s_i)&= const+ \mathbb{E}\log\frac{\pi_{g(i)}}{1-\pi_{g(i)}} s_i +\frac{1}{2} \log{\sigma_i^2(s_i)} +\frac{\mu_i(s_i)^2}{2\sigma_i^2(s_i)} \\
&= const+  s_i  \mathbb{E}\log\frac{\pi_{g(i)}}{1-\pi_{g(i)}} -\frac{1}{2}  \log(s_i\mathbb{E} \tau ||X_{\cdot,i}||_2^2 +\mathbb{E}\gamma_{g(i)})
+\frac{s_i^2 ( \mathbb{E} \tau)^2  \left(X_{\cdot,i}^T y -\sum_{k=1}^n \sum_{j\neq i}^p X_{ki} X_{kj} \mathbb{E} (\tilde{\beta}_j s_j) \right)^2}{2 (s_i\mathbb{E} \tau ||X_{\cdot,i}||_2^2 +\mathbb{E}\gamma_{g(i)})}\\
&=const +s_i \left\{\mathbb{E}\log\frac{\pi_{g(i)}}{1-\pi_{g(i)}} -  
\frac{1}{2}  \log(\mathbb{E} \tau ||X_{\cdot,i}||_2^2 +\mathbb{E}\gamma_{g(i)}) + \frac{1}{2}  \log(\mathbb{E}\gamma_{g(i)}) +
\frac{( \mathbb{E} \tau)^2  \left(X_{\cdot,i}^T y -\sum_{k=1}^n \sum_{j\neq i}^p X_{ki} X_{kj} \mathbb{E} (\tilde{\beta}_j s_j) \right)^2}{2 (\mathbb{E} \tau ||X_{\cdot,i}||_2^2 +\mathbb{E}\gamma_{g(i)})} \right\}
\end{align}
$$
in the last steps note $s\in \{0,1\}$. Comparing this to $s\sim Ber(\psi)$ where $\log(q(s))=const + (s\, logit(\psi))$ we get the above formula.


Taken together, $\beta_i=s_i*\tilde{\beta_i} \sim \delta_0 (1-\psi_i) + \psi_i N(\mu_i(1),\sigma_i^2(1))$

### Other updates
The other updates are not affected, except that the expected value of $||y-X\beta||^2$ simplifies due to the independence of $\beta_i$.

## Expected values required
$$
\begin{align}
 \mathbb{E} \tau &= \frac{\alpha^{\tau}}{\beta^{\tau}}\\
 \mathbb{E}\gamma_{k} &= \frac{\alpha^{\gamma,k}_l}{\beta^{\gamma,k}_l}\\
 \mathbb{E}\log\frac{\pi_k}{1-\pi_k} & =  \psi(\alpha^{\pi}) - \psi(\beta^{\pi})\\
 \mathbb{E} s_i & = \psi_i\\
 \mathbb{E}_\beta||y-X\beta||^2_2 &=y^t y -2 y^t X \mu_\beta + \sum_{i,j} (X^tX)_{i,j} (\Sigma^\beta_{i,j} +\mu^\beta_{i}\mu^\beta_{j})   \\
 \mathbb{E}_\tilde{\beta}(\tilde{\beta}_i^2) &= (1-\psi) \left(\mu_i(s=0)^2 + \sigma_i^2(s=0)\right) + \psi\left(\mu_i(s=1)^2 + \sigma_i^2(s=1)\right)\\
  \mathbb{E} \beta_i = \mathbb{E} \tilde{\beta}_i s_i &=\mu_i(s=1) \psi\\
  \mathbb{E} \beta_i^2 = \mathbb{E} \tilde{\beta}_i^2 s_i &=(\mu_i(s=1)^2 +  \sigma_i^2(s=1)) \psi \\
  \end{align}
$$
where $\psi$ dneotes the digamam function and $\mu^\beta = (\mathbb{E} \beta_i)_i = (\mathbb{E} \tilde{\beta}_i s_i)_i$ adn $\Sigma^\beta= diag(Var(\beta_i))=diag( \mathbb{E} \beta_i^2 - (\mathbb{E} \beta_i)^2)$.

## Evidence Lower Bound
The evidence lower bound or "free energy" bounds the log model evidence from below and can be calculated in each step to observe convergence. Recall
$$\log(y)=\mathcal{L}(q)+KL(q||p)$$
with
$$
\begin{align}
\mathcal{L}(q)&=\mathbb{E}_q \left(\log \frac{p(y, \beta, \gamma, \tau, \pi)}{q(\beta, \gamma, \tau, 
\pi)}\right)\\
&=\mathbb{E}_q \left(\log p(y, \tilde{\beta}, s, \gamma, \tau, \pi)\right) +H(q(\tilde{\beta}, s, \gamma, \tau))\\
&=\mathbb{E}_q \left(\log p(y, \tilde{\beta}, s, \gamma, \tau, \pi)\right) + \sum_{i=1}^p H(q(\tilde{\beta_i}, s_i)) + H(q(\gamma))+ H(q(\pi)) +H(q(\tau))
\end{align}
$$
where $H(q)=\int - q(\theta) \log q(\theta) d\theta$ denotes the differential entropy.

####joint density
$$
\begin{align}
\mathbb{E}_q \log p(y, \tilde{\beta}, s, \gamma, \tau, \pi)&=\mathbb{E}_q \log p(y| \beta, \tau) + \mathbb{E}_q \log p(\tilde{\beta}| \gamma)+ \mathbb{E}_q \log p(s| \pi) +\mathbb{E}_q \log p(\gamma)+\mathbb{E}_q \log p(\pi)+\mathbb{E}_q \log p(\tau)
\end{align}
$$
with
$$
\begin{align}
\mathbb{E}_q \log p(y| \beta) &= \frac{n}{2} \mathbb{E} \log(\tau) -\frac{1}{2}\mathbb{E}\tau ||y-X\beta||_2^2-\frac{n}{2} \log(2\pi)\\
\mathbb{E}_q \log p(\tilde{\beta}| \gamma) &= \sum_i \frac{1}{2} \mathbb{E} \log(\gamma_{g(i)}) - \frac{1}{2} \mathbb{E} \gamma_{g(i)}\tilde{\beta_i}^2- \frac{1}{2} \log(2\pi)\\ 
\mathbb{E}_q \log p(s| \pi) &= \sum_i  \mathbb{E} (s_i \log(\pi_{g(i)})) +\mathbb{E} ((1-s_i) \log(1-\pi_{g(i)}))\\ 
\mathbb{E}_q \log p(\pi) &=\sum_k (d_\pi-1)\mathbb{E} \log(\pi_k)+(r_\pi-1)\mathbb{E} \log(1-\pi_k) -\log(B(d_\pi,r_\pi))   \\
\mathbb{E}_q \log p(\gamma) &=\sum_k (r_\gamma-1)\mathbb{E} \log(\gamma_k) - d_\gamma \mathbb{E} \gamma_k - \log(\Gamma((r_\gamma)) + r_\gamma \log(d_\gamma) \\
\mathbb{E}_q \log p(\tau) &=(r_\tau-1)\mathbb{E} \log(\tau) - d_\tau \mathbb{E} \tau - \log(\Gamma((r_\tau)) + r_\gamma \log(d_\tau)
\end{align}
$$

The expectation factroize as q is assumed to facotrize, hence using the known ditirbutions and parameters of the variational density in each iteration one gets
$$
\begin{align}
\mathbb{E} \log(\tau)&=\psi(\alpha_\tau) - \log(\beta_\tau)\\
\mathbb{E}\tau ||y-X\beta||_2^2=\mathbb{E}\tau \mathbb{E}||y-X\beta||_2^2&=\frac{\alpha^{\tau}_l}{\beta^{\tau}_l}(y^t y -2 y^t X \mu_l + \sum_{i,j} (X^tX)_{i,j} (\Sigma^{\beta}_{i,j} +\mu^{\beta}_{i}\mu^{\beta}_{j}) )\\
\mathbb{E} \log(\gamma_{g(i)})&=\psi(\alpha_{\gamma,g(i)}) - \log(\beta_{\gamma,g(i)})\\
\mathbb{E} \gamma_{g(i)}\tilde{\beta_i}^2 &=\mathbb{E} \gamma_{g(i)}\mathbb{E}\tilde{\beta_i}^2\\
\mathbb{E} \gamma_k&=\frac{\alpha^{\gamma,k}_l}{\beta^{\gamma,k}_l}\\
\mathbb{E} \tau&=\frac{\alpha^{\tau}_l}{\beta^{\tau}_l}\\
\mathbb{E} \log(\pi_k) &= \psi(\alpha_\pi)-\psi(\alpha_\pi+\beta_\pi)\\
\mathbb{E} \log(1-\pi_k) &=\psi(\beta_\pi)-\psi(\alpha_\pi+\beta_\pi)
\end{align}
$$

####entropies
Using the known expression for entropy (with the natural logarithm) of the gamma and multivaraite normal distribution one obtains
$$
\begin{align}
H(q(\tilde{\beta_i},s_i)) &=\mathbb{E} H(q(\tilde{\beta_i}|s_i)) +H(q(s_i))\\
&= \frac{1}{2} (\log(2\pi)+1) -\frac{1}{2} \mathbb{E}\log(s_i \mathbb{E} \tau ||X_{\cdot,i}||_2^2 +\mathbb{E}\gamma_{g(i)})
-(1-\psi_i)\log(1-\psi_i)-\psi_i\log(\psi_i)\\
&= \frac{1}{2} (\log(2\pi)+1) -\frac{1}{2} \psi_i \log(\mathbb{E} \tau ||X_{\cdot,i}||_2^2 +\mathbb{E}\gamma_{g(i)}) -\frac{1}{2} (1-\psi_i) \log(\mathbb{E}\gamma_{g(i)}) -(1-\psi_i)\log(1-\psi_i)-\psi_i\log(\psi_i)\\
H(q(\gamma)) &=\sum_k \alpha^{\gamma,k}_l - \log(\beta^{\gamma,k}_l) + \log (\Gamma(\alpha^{\gamma,k}_l)) + (1-\alpha^{\gamma,k}_l)\psi(\alpha^{\gamma,k}_l)\\
H(q(\tau)) &=\alpha^{\tau}_l - \log(\beta^{\tau}_l) + \log (\Gamma(\alpha^{\tau}_l)) + (1-\alpha^{\tau}_l)\psi(\alpha^{\tau}_l)\\
H(q(\pi)) &= \log(B(\alpha_\pi, \beta_\pi)) - (\alpha_\pi-1)\psi(\alpha_\pi)-(\beta_\pi-1)\psi(\beta_\pi)+(\alpha_\pi+\beta_\pi-2) \psi(\alpha_\pi +\beta_\pi)
\end{align}
$$
where $\psi$ denotes the digamma function $\psi(x)=\frac{\Gamma'(x)}{\Gamma(x)}$ and $B$ the Beta function $B(a,b)=\frac{\Gamma (a)\,\Gamma (b)}{\Gamma (a+b)}$.




#Extension of VB with normal prior to a logisitc regression framework


## Hierarchical model
If the response is binary a corresponding hierarchical model could be
$$
\begin{align}
\gamma_k &\sim \Gamma(r_\gamma, d_\gamma) \quad \forall k\\
\beta_i|\gamma_i &\sim N(0, \frac{1}{\gamma_g(i)})\quad \forall i\\
y|\beta &\sim Ber(\sigma(X\beta)) \quad \sigma(a)=\frac{1}{1+\exp(-a)}
\end{align}
$$

(see also chapter 10.6.1 in Bishop (2006))

##Approximation of logit
As $\gamma$ does not directly enter the distribution of $y|\beta$ the updates for $\gamma$ are identical as in the linear regression.
The Bernoulli distribution on the data does not allow for a conjugate prior as in the linear model but using the approximation below the sigmoid function can be approximated by an exponential of a quadratic term making a normal prior convenient.
Maiking use of $\sigma(-a) = 1- \sigma(a)$ the we can write
$$
\begin{align}
\mathbb{P}(y=1|\beta)&=\sigma(X\beta) \\
\mathbb{P}(y=0|\beta)&=\sigma(-X\beta) \\
\Rightarrow p(y|\beta)&=\sigma((2y-1)X\beta)
\end{align}
$$
(or with $y \in \{-1,1\}$ $p(y|\beta)=\sigma((yX\beta)$).

Jaakkola proposed the following lower bound on the sigmoid being of convenient form making a normal prior conjugate (https://arxiv.org/pdf/1310.5438v2.pdf, Bishop, http://link.springer.com/article/10.1023/A:1008932416310)

$$\sigma(z)\geq \sigma(\xi)\exp(\frac{1}{2}(z-\xi)-\lambda(\xi)(z^2-\xi^2)) \qquad \lambda(\xi)=\frac{1}{2\xi}(\sigma(\xi)-\frac{1}{2})$$
(general concept: local variational methods, log of sigmoid is concave, use of transformation beforehand gives a gaussian form, see 10.5 Bishop for detils)

## Joint density
With this $\log p(y|\beta)=\sum_i \log(\sigma(y_iX_i\beta))$ can be bounded as follows (note $(2y-1)^2=1$)
$$
\begin{align}
\log p(y|\beta) &\geq \log h(\beta, \xi)\\
&=\frac{1}{2}\sum_i (2y_i-1) X_i \beta -\sum_i \lambda(\xi_i) (X_i \beta)^2   +\sum_i \left( \log(\sigma(\xi_i))-\frac{1}{2}\xi_i +\lambda(\xi_i) \xi_i^2 \right)
\end{align}
$$

giving rise to the following joint density appproximation
$$\begin{align}
p(y,\beta,\gamma)&=p(y|\beta)p(\beta|\gamma)p(\gamma)\\
&\geq h(\beta, \xi)
\left(\prod_{i=1}^p \frac{\sqrt{\gamma_{g(i)}}}{\sqrt{2\pi}} \exp\left(-\frac{\gamma_{g(i)}}{2}\beta_i^2\right) \right)
\left(\prod_{k=1}^g \frac{d_\gamma^{r_\gamma}}{\Gamma(r_\gamma)} \gamma_k^{r_\gamma-1} \exp(-d_\gamma \gamma_k)\right)
\end{align}
$$

## VB updates
This result in the following variational updates

###For $\beta$:
$$\begin{align}
\log q(\beta)&=const +\log h(\beta, \xi)+
\sum_{i=1}^p \left\{-\frac{\mathbb{E}_\gamma(\gamma_{g(i)})}{2}\beta_i^2 \right\}\\
&= const \frac{1}{2}\sum_i (2y_i-1) X_i \beta -\sum_i \lambda(\xi_i) (X_i \beta)^2 +
\sum_{i=1}^p \left\{-\frac{\mathbb{E}_\gamma(\gamma_{g(i)})}{2}\beta_i^2 \right\}\\
&= const \left(\sum_i (y_i-\frac{1}{2})X_i\right)  \beta -\beta^T \left(\sum_i \lambda(\xi_i) X_i^T X_i\right) \beta +
\sum_{i=1}^p \left\{-\frac{\mathbb{E}_\gamma(\gamma_{g(i)})}{2}\beta_i^2 \right\}
\end{align}
$$
(Rewrite $X_i \beta=(X_i \beta)^T= \beta^T X_i^T$, $X_i$ denoting the i-th row of X$, $X_i^T X_i$ pxp outer product matrix.)
Thus, $\beta \sim N(\mu_l, \Sigma_l)$ is a normal distribution with parameters

$$\begin{align}
\mu_l &= \Sigma^{(l)} \sum_i (y_i-\frac{1}{2})X_i^T\\
\Sigma^{(l)} &= \left(2(\sum_i \lambda(\xi_i) X_i^T X_i)+D\right)^{-1} \quad \text{with}\, D=diag(\mathbb{E}_\gamma(\gamma_{g(i)}))_i
\end{align}
$$

###For $\gamma$:
$$\begin{align}
\log q(\gamma)&=const+ 
\sum_{i=1}^p \left\{ \frac{1}{2}\log(\gamma_{g(i)}) -\frac{\gamma_{g(i)}}{2}\mathbb{E}_\beta(\beta_i^2) \right\}+
\sum_{k=1}^g \left\{(r_\gamma-1)  \log(\gamma_k ) -d_\gamma \gamma_k \right\}\\
&=const+\sum_{k=1}^g \left\{  \log(\gamma_k )  (r_\gamma-1 +\frac{1}{2}|\mathcal{G}_k|) - \gamma_k (d_\gamma+\frac{1}{2}\sum_{i\in\mathcal{G}_k}\mathbb{E}_\beta(\beta_i^2))\right\}
\end{align}
$$
Thus $\gamma_k \sim \Gamma(\alpha^{\gamma,k}_l, \beta^{\gamma,k}_l)$ are independent Gamma distributions with parameters given by
$$\begin{align}
\alpha^{\gamma,k}_l &= r_\gamma +\frac{1}{2}|\mathcal{G}_k|\\
\beta^{\gamma,k}_l &= d_\gamma+\frac{1}{2}\sum_{i\in\mathcal{G}_k}\mathbb{E}_\beta(\beta_i^2)
\end{align}
$$

###Update variational parameter $\xi$
$$\xi_i^2=X_i(\Sigma^{(l)}+ \mu_l\mu_l^T) X_i^T$$
(simply EM: derivation in Appendix A: parameters Optimization of the variational of http://link.springer.com/article/10.1023/A:1008932416310)

Restirct to nonegative value w.l.o.g. due to symmetry.

##Required expected values
$$
\begin{align}
\mathbb{E}_\gamma{\gamma_{k}}&=\frac{\alpha^{\gamma,k}_l}{\beta^{\gamma,k}_l}\\
&=\frac{r_\gamma +\frac{1}{2}|\mathcal{G}_k|}{d_\gamma+\frac{1}{2}\sum_{i\in\mathcal{G}_k}\mathbb{E}_\beta(\beta_i^2)}\\
\\
\mathbb{E}_\beta{\beta_i^2} &= \mu_l^2+\Sigma^{(l)}_{ii}\\
&=(\mathbb{E}_\tau(\tau)\Sigma^{(l)} X^t y)^2 +(\mathbb{E}_\tau(\tau) X^t X +D)^{-1}_{ii}
\end{align}
$$

##Evidence lower bound
...

## A fully factorised version
As in the linear model we make a rougher assumption of full factorization in posterior density of beta.
The the updates are

###For beta
Factorizing the variational density of $\beta$ fully lead to the follwing distributions in each update, but still assume full factorization.

$$
\begin{align}
\beta_i \sim N(\mu_i, \sigma^2_i)
\end{align}
$$
with
$$
\begin{align}
\sigma_i^2&=(2 \sum_{k=1}^n \lambda(\xi_k) X_{ki}^2 +\mathbb{E}\gamma_{g(i)})^{-1}\\
\mu_i&=\sigma_i^2 \left(-2 \sum_{k=1}^n \lambda(\xi_k) \sum_{j\neq i}^p X_{ki}X_{kj} \mathbb{E} \beta_j + 0.5 X^t_s(2y-1)\right)
\end{align}
$$

The other updates stay the same.

#Session Info
```{r}
sessionInfo()
```

